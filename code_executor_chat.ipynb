{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import AssistantAgent, GroupChatManager, GroupChat, config_list_from_json\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent, TEXT_FORMATS\n",
    "\n",
    "from CypherExecutor import CypherCodeExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted formats for \"docs_path\": \n",
      "['txt', 'json', 'csv', 'tsv', 'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml', 'pdf']\n"
     ]
    }
   ],
   "source": [
    "# This list will 'connect' to the llama3.1 model running on llm container\n",
    "config_list = config_list_from_json(env_or_file=\"CONFIG_LIST\", filter_dict={\"model\": \"llama3.1\"})\n",
    "\n",
    "\n",
    "llm_config = {\"config_list\": config_list, \"temperature\": 0.2}\n",
    "\n",
    "prompt = \"\"\"You are a data scientist that works with Cypher queries.\n",
    "All you have to do is translate the given answer as Cypher queries.\n",
    "\n",
    "You have to respect this rules:\n",
    "- You can use the pdf files. \n",
    "- You must generate the easiest query possible in cypher format.\n",
    "- Every query must be in a separate cypher format.\n",
    "- You must instert all the information you have and where you found them.\n",
    "- You must be precise. \n",
    "- If the query is runned without errors you can send 'TERMINATE'.\n",
    "\n",
    "Now i will give you some information about the database schema.\n",
    "- nodes -\n",
    "(:Movie), Describe a movie that has a title and a plot. It can also have the number of likes.\n",
    "(:Person), Describe actors and directors. They have a name, a birthday.\n",
    "\n",
    "- relationships -\n",
    "(:Person) -[:ACTED_IN]-> (:Movie)\n",
    "\n",
    "QUESTION IS:\n",
    "{input_question}\n",
    "\"\"\"\n",
    "\n",
    "print(f'Accepted formats for \"docs_path\": \\n{TEXT_FORMATS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termination_msg(x):\n",
    "    return isinstance(x, dict) and \"TERMINATE\" == str(x.get(\"content\", \"\"))[-9:].upper()\n",
    "\n",
    "doc_retriever = RetrieveUserProxyAgent(\n",
    "    name=\"doc_retriever\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    max_consecutive_auto_reply=3,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    retrieve_config={\n",
    "        \"task\": \"code\",\n",
    "        \"docs_path\": \"https://s3.amazonaws.com/artifacts.opencypher.org/openCypher9.pdf\",\n",
    "        \"model\": config_list[0][\"model\"],\n",
    "        \"get_or_create\": True,\n",
    "        \"customized_prompt\": prompt\n",
    "    },\n",
    "    code_execution_config=False,\n",
    "    description=\"Assistant who has extra content retrieval power for solving difficult problems.\"\n",
    ")\n",
    "\n",
    "coder      = AssistantAgent(\n",
    "    name=\"coder\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    system_message=prompt,\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "executor   = AssistantAgent(\n",
    "    name=\"executor\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    description= \"\"\"Executor provides feedback based on the errors and warning generated by the query.\"\"\",\n",
    "    code_execution_config={\"executor\": CypherCodeExecutor()}\n",
    "    )\n",
    "\n",
    "\n",
    "PROBLEM  = \"Who is the most prolific actor?\"\n",
    "\n",
    "def _reset_agents():\n",
    "    \"\"\"\n",
    "    This function reset all the agents used for the group chat.\n",
    "    This should be used every time you start a new conversation.\n",
    "    \"\"\"\n",
    "    doc_retriever.reset()\n",
    "    coder.reset()\n",
    "    executor.reset()\n",
    "\n",
    "#TODO: find why match/case not working \n",
    "def state_transition(last_speaker, groupchat):\n",
    "    \"\"\"\n",
    "    This function simply return the next speaker based on the last one.\n",
    "    In this case we decided that doc_retriever should speak only to initiate chat\n",
    "    and then the chat is from coder to executor in a sort of loop.\n",
    "    \"\"\"\n",
    "    if last_speaker is coder: return executor\n",
    "    elif last_speaker is executor: return coder\n",
    "    elif last_speaker is doc_retriever: return coder\n",
    "\n",
    "def rag_chat():\n",
    "    _reset_agents()\n",
    "    groupchat = GroupChat(agents=[doc_retriever, coder, executor], messages=[], \n",
    "                          max_round=5, speaker_selection_method=state_transition)\n",
    "    manager   = GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "\n",
    "    doc_retriever.initiate_chat(manager,message=doc_retriever.message_generator, problem=PROBLEM,n_results=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create collection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 09:31:31,409 - autogen.agentchat.contrib.retrieve_user_proxy_agent - INFO - \u001b[32mUse the existing collection `autogen-docs`.\u001b[0m\n",
      "max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t          ...\n",
      "Failed to split docs with must_break_at_empty_line being True, set to False.\n",
      "2024-09-10 09:31:36,017 - autogen.agentchat.contrib.retrieve_user_proxy_agent - INFO - Found 287 chunks.\u001b[0m\n",
      "Model llama3.1 not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorDB returns doc_ids:  [['34d3d3db', 'da9c8a26', 'c3fdf098']]\n",
      "\u001b[32mAdding content of doc 34d3d3db to context.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3.1 not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mAdding content of doc da9c8a26 to context.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3.1 not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mAdding content of doc c3fdf098 to context.\u001b[0m\n",
      "\u001b[33mdoc_retriever\u001b[0m (to chat_manager):\n",
      "\n",
      "You are a data scientist that works with Cypher queries.\n",
      "All you have to do is translate the given answer as Cypher queries.\n",
      "\n",
      "You have to respect this rules:\n",
      "- You can use the pdf files. \n",
      "- You must generate the easiest query possible in cypher format.\n",
      "- Every query must be in a separate cypher format.\n",
      "- You must instert all the information you have and where you found them.\n",
      "- You must be precise. \n",
      "- If the query is runned without errors you can send 'TERMINATE'.\n",
      "\n",
      "Now i will give you some information about the database schema.\n",
      "- nodes -\n",
      "(:Movie), Describe a movie that has a title and a plot. It can also have the number of likes.\n",
      "(:Person), Describe actors and directors. They have a name, a birthday.\n",
      "\n",
      "- relationships -\n",
      "(:Person) -[:ACTED_IN]-> (:Movie)\n",
      "\n",
      "QUESTION IS:\n",
      "Who is the most prolific actor?\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: coder\n",
      "\u001b[0m\n",
      "[autogen.oai.client: 09-10 09:34:37] {329} WARNING - Model llama3.1 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mcoder\u001b[0m (to chat_manager):\n",
      "\n",
      "To find the most prolific actor, we need to count the number of movies each actor has acted in. We can do this by running the following Cypher query:\n",
      "\n",
      "```cypher\n",
      "MATCH (p:Person)-[:ACTED_IN]->(m:Movie)\n",
      "RETURN p.name AS name, COUNT(m) AS num_movies\n",
      "ORDER BY num_movies DESC\n",
      "LIMIT 1\n",
      "```\n",
      "\n",
      "This query matches all actors who have acted in movies, counts the number of movies each actor has acted in, and returns the actor with the highest count. The `ORDER BY` clause sorts the results by the count in descending order, and the `LIMIT 1` clause ensures that we only get the top result.\n",
      "\n",
      "Please run this query without errors and let me know if it works!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: executor\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK (inferred language is cypher)...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'Robert Downey Jr.', 'num_movies': 3}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mexecutor\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: coder\n",
      "\u001b[0m\n",
      "[autogen.oai.client: 09-10 09:35:07] {329} WARNING - Model llama3.1 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mcoder\u001b[0m (to chat_manager):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "rag_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
