{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import AssistantAgent, GroupChatManager, GroupChat, config_list_from_json\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent, TEXT_FORMATS\n",
    "\n",
    "from CypherExecutor import CypherCodeExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted formats for \"docs_path\": \n",
      "['txt', 'json', 'csv', 'tsv', 'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml', 'pdf']\n"
     ]
    }
   ],
   "source": [
    "# This list will 'connect' to the llama3.1 model running on llm container\n",
    "config_list = config_list_from_json(env_or_file=\"CONFIG_LIST\", filter_dict={\"model\": \"llama3.1\"})\n",
    "\n",
    "\n",
    "llm_config = {\"config_list\": config_list, \"temperature\": 0.2}\n",
    "\n",
    "prompt = \"\"\"You are a data scientist that works with Cypher queries.\n",
    "Your work is to translate the given answer as Cypher queries.\n",
    "\n",
    "You have some rules to respect:\n",
    "- You must use cypher language.\n",
    "- You can use the pdf files. \n",
    "- You must generate the easiest cypher query possible.\n",
    "- Every query must be in a separate cypher format.\n",
    "- You must be precise and explain everything, code outputs too. \n",
    "- If the code runs successfully you can send 'TERMINATE'.\n",
    "\n",
    "Now i will give you some information about the database schema.\n",
    "- nodes -\n",
    "(:Movie), Describe a movie that has a title and a plot. It can also have the number of likes.\n",
    "(:Person), Describe actors and directors. They have a name, a birthday.\n",
    "\n",
    "- relationships -\n",
    "(:Person) -[:ACTED_IN]-> (:Movie)\n",
    "\n",
    "QUESTION IS:\n",
    "{input_question}\n",
    "\"\"\"\n",
    "\n",
    "print(f'Accepted formats for \"docs_path\": \\n{TEXT_FORMATS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cate/Scrivania/scuola/uni/traineeship/madt-mlagents/.venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/cate/Scrivania/scuola/uni/traineeship/madt-mlagents/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def termination_msg(x):\n",
    "    return isinstance(x, dict) and \"TERMINATE\" == str(x.get(\"content\", \"\"))[-9:].upper()\n",
    "\n",
    "doc_retriever = RetrieveUserProxyAgent(\n",
    "    name=\"doc_retriever\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    max_consecutive_auto_reply=3,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    retrieve_config={\n",
    "        \"task\": \"code\",\n",
    "        \"docs_path\": \"https://s3.amazonaws.com/artifacts.opencypher.org/openCypher9.pdf\",\n",
    "        \"model\": config_list[0][\"model\"],\n",
    "        \"get_or_create\": True,\n",
    "        \"customized_prompt\": prompt\n",
    "    },\n",
    "    code_execution_config=False,\n",
    "    description=\"Assistant who has extra content retrieval power for solving difficult problems.\"\n",
    ")\n",
    "\n",
    "coder      = AssistantAgent(\n",
    "    name=\"coder\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    system_message=prompt,\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "executor   = AssistantAgent(\n",
    "    name=\"executor\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    description= \"\"\"Executor provides feedback based on the errors and warning generated by the query.\"\"\",\n",
    "    code_execution_config={\"executor\": CypherCodeExecutor()}\n",
    "    )\n",
    "\n",
    "\n",
    "PROBLEM  = \"Who directed Matrix?\"\n",
    "\n",
    "def _reset_agents():\n",
    "    \"\"\"\n",
    "    This function reset all the agents used for the group chat.\n",
    "    This should be used every time you start a new conversation.\n",
    "    \"\"\"\n",
    "    doc_retriever.reset()\n",
    "    coder.reset()\n",
    "    executor.reset()\n",
    "\n",
    "#TODO: find out why match/case not working \n",
    "def state_transition(last_speaker, groupchat):\n",
    "    \"\"\"\n",
    "    This function simply return the next speaker based on the last one.\n",
    "    In this case we decided that doc_retriever should speak only to initiate chat\n",
    "    and then the chat is from coder to executor in a sort of loop.\n",
    "    \"\"\"\n",
    "    if last_speaker is coder: return executor\n",
    "    elif last_speaker is executor: return coder\n",
    "    elif last_speaker is doc_retriever: return coder\n",
    "\n",
    "def rag_chat():\n",
    "    _reset_agents()\n",
    "    groupchat = GroupChat(agents=[doc_retriever, coder, executor], messages=[], \n",
    "                          max_round=5, speaker_selection_method=state_transition)\n",
    "    manager   = GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "\n",
    "    doc_retriever.initiate_chat(manager,message=doc_retriever.message_generator, problem=PROBLEM,n_results=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 12:11:38,775 - autogen.agentchat.contrib.retrieve_user_proxy_agent - INFO - \u001b[32mUse the existing collection `autogen-docs`.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create collection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\t          ...\n",
      "Failed to split docs with must_break_at_empty_line being True, set to False.\n",
      "2024-09-11 12:11:46,815 - autogen.agentchat.contrib.retrieve_user_proxy_agent - INFO - Found 287 chunks.\u001b[0m\n",
      "Model llama3.1 not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorDB returns doc_ids:  [['bb85fb1c', 'fd79e570', 'c3fdf098']]\n",
      "\u001b[32mAdding content of doc bb85fb1c to context.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3.1 not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mAdding content of doc fd79e570 to context.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3.1 not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mdoc_retriever\u001b[0m (to chat_manager):\n",
      "\n",
      "You are a data scientist that works with Cypher queries.\n",
      "Your work is to translate the given answer as Cypher queries.\n",
      "\n",
      "You have some rules to respect:\n",
      "- You must use cypher language.\n",
      "- You can use the pdf files. \n",
      "- You must generate the easiest cypher query possible.\n",
      "- Every query must be in a separate cypher format.\n",
      "- You must be precise and explain everything, code outputs too. \n",
      "- If the code runs successfully you can send 'TERMINATE'.\n",
      "\n",
      "Now i will give you some information about the database schema.\n",
      "- nodes -\n",
      "(:Movie), Describe a movie that has a title and a plot. It can also have the number of likes.\n",
      "(:Person), Describe actors and directors. They have a name, a birthday.\n",
      "\n",
      "- relationships -\n",
      "(:Person) -[:ACTED_IN]-> (:Movie)\n",
      "\n",
      "QUESTION IS:\n",
      "Who directed Matrix?\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: coder\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "rag_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
