services:
  
  graph:
    image: neo4j:enterprise
    ports:
      - "7474:7474"
      - "7687:7687"
    container_name: graph
    environment:
      NEO4J_PLUGINS: '["graph-data-science","apoc", "apoc-extended"]'
      NEO4J_AUTH: "none"
      NEO4J_ACCEPT_LICENSE_AGREEMENT: "yes"
      NEO4J_apoc_export_file_enabled: "true" 
      NEO4J_apoc_import_file_enabled: "true" 
      NEO4J_dbms_security_procedures_allowlist: gds.*,apoc.*
      NEO4J_dbms_security_procedures.unrestricted: apoc.*, gds.*
      NEO4J_apoc_import_file_use__neo4j__config: "false"
      #NEO4J_db_format: "high_limit" # Usefull if there is a large db, only for enterprise
    volumes:
      - ./movies.cypher:/var/lib/neo4j/files/movies.cypher
    healthcheck:
      test: wget http://localhost:7474 && bin/cypher-shell -u neo4j -p useruser --fail-fast -f files/movies.cypher || exit 1
      start_period: 30s 
      interval: 40s
      retries: 20
    tty: true
    depends_on:
      llm:
        condition: service_healthy

  # Ollama container 
  llm:
    image: ollama/ollama
    # Container name is used to access ollama services from the notebook
    container_name: llm
    ports:
      - 11434:11434
    # Let ollama run on GPU, if you do not have one we suggest to comment/remove this part.
    # The documentation to install the correct drivers can be found at https://hub.docker.com/r/ollama/ollama.
    healthcheck:
      test: ollama pull llama3.1
      timeout: 10m
      retries: 3
    extra_hosts:
      - "host.docker.internal:host-gateway"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    tty: true
    volumes: # Ollama stores the installed model 
      - ollama:/root/.ollama
    restart: unless-stopped

volumes:
  ollama:
