{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMPLE AGENT\n",
    "\n",
    "This simple agent is similar to the tool agent (both of them are based on their past knowledge).\n",
    "The difference is that this agent doesn't have a  tool and tries to generate a cypher query to retrieve the data to answer the question.\n",
    "The query is not executed, this was a first step to test how prompt can be useful too.\n",
    "If you don't provide, for example, the schema of the database the generated query can be general and should be changed to be run in a real database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent, register_function, config_list_from_json\n",
    "from tools import * # Imports all the functions\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using a llama3.1 local model \n",
    "config_list = config_list_from_json(env_or_file=\"CONFIG_LIST\", filter_dict={\"model\": \"gpt-4o\"})\n",
    "config_list[0][\"api_key\"] = os.environ.get(\"GITHUB_TOKEN\")\n",
    "\n",
    "# This simple prompt will give some advice to the model on how to do things\n",
    "prompt = \"\"\"You are a data scientist that works with Cypher queries.\n",
    "All you have to do is translate the given answer as Cypher queries.\n",
    "\n",
    "You have to respect this rules:\n",
    "- You must generate the easiest query possible in cypher format.\n",
    "- You must instert all the information you have and where you found them.\n",
    "- You must be precise. \n",
    "- You must re-generate the query if you think it could generates some errors.\n",
    "\n",
    "When you think the query can be run you can submit it as the final answer.\n",
    "When you have written your final answer you must send 'TERMINATE'.\n",
    "\n",
    "Now i will give you some information about the database schema.\n",
    "- nodes -\n",
    "(:Movie), Describe a movie that has a title and a plot. It can also have the number of likes.\n",
    "(:Person), Describe actors and directors. They have a name, a birthday and they may also have the death date.\n",
    "\n",
    "- relationships -\n",
    "(:Movie) <-[:DIRECTED]- (:Person)\n",
    "(:Person) -[:ACTED-IN]-> (:Movie)\n",
    "(:Person) -[:KNOWS]-> (:Person)\n",
    "(:Movie) -[:type]-> (:Genre)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mquestion\u001b[0m (to coder):\n",
      "\n",
      "What is the title of a movie that talks about romance?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "[autogen.oai.client: 09-06 14:06:30] {329} WARNING - Model llama3.1 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mcoder\u001b[0m (to question):\n",
      "\n",
      "Here is the simplest Cypher query possible:\n",
      "```markdown\n",
      "MATCH (m:Movie {title: null})-[:type]->(g:Genre)<-[:type]-(gm:{name: \"Romance\"})\n",
      "RETURN m.title AS Title\n",
      "```\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mquestion\u001b[0m (to coder):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "[autogen.oai.client: 09-06 14:07:50] {329} WARNING - Model llama3.1 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mcoder\u001b[0m (to question):\n",
      "\n",
      "Wait... I made a small mistake. We want to query movies with romance genre, not the genres themselves.\n",
      "\n",
      "Here is the corrected Cypher query:\n",
      "```markdown\n",
      "MATCH (m:Movie {title: null})-[:type]->(g:Genre {name: \"Romance\"})-[:type]-(tg)\n",
      "RETURN m.title AS Title\n",
      "```\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': 'What is the title of a movie that talks about romance?', 'role': 'assistant', 'name': 'question'}, {'content': 'Here is the simplest Cypher query possible:\\n```markdown\\nMATCH (m:Movie {title: null})-[:type]->(g:Genre)<-[:type]-(gm:{name: \"Romance\"})\\nRETURN m.title AS Title\\n```\\nTERMINATE', 'role': 'user', 'name': 'coder'}, {'content': '', 'role': 'assistant', 'name': 'question'}, {'content': 'Wait... I made a small mistake. We want to query movies with romance genre, not the genres themselves.\\n\\nHere is the corrected Cypher query:\\n```markdown\\nMATCH (m:Movie {title: null})-[:type]->(g:Genre {name: \"Romance\"})-[:type]-(tg)\\nRETURN m.title AS Title\\n```\\nTERMINATE', 'role': 'user', 'name': 'coder'}], summary='Wait... I made a small mistake. We want to query movies with romance genre, not the genres themselves.\\n\\nHere is the corrected Cypher query:\\n```markdown\\nMATCH (m:Movie {title: null})-[:type]->(g:Genre {name: \"Romance\"})-[:type]-(tg)\\nRETURN m.title AS Title\\n```\\n', cost={'usage_including_cached_inference': {'total_cost': 0, 'llama3.1': {'cost': 0, 'prompt_tokens': 499, 'completion_tokens': 133, 'total_tokens': 632}}, 'usage_excluding_cached_inference': {'total_cost': 0, 'llama3.1': {'cost': 0, 'prompt_tokens': 499, 'completion_tokens': 133, 'total_tokens': 632}}}, human_input=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This agent will start the conversation asking a simple question\n",
    "question = ConversableAgent(\n",
    "    \"question\",\n",
    "    llm_config=False,\n",
    "    code_execution_config=False,\n",
    ")\n",
    "\n",
    "# This agent will try to answer the question \n",
    "coder = ConversableAgent(\n",
    "    \"coder\",\n",
    "    system_message = prompt,\n",
    "    llm_config = {\"config_list\": config_list},\n",
    "    code_execution_config=False\n",
    ")\n",
    "\n",
    "# This will ask the driver to answer to one question \n",
    "question.initiate_chat(\n",
    "    coder,\n",
    "    message = \"What is the title of a movie that talks about romance?\",\n",
    "    max_turns = 2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
